{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b082671f",
   "metadata": {},
   "source": [
    "# CONVERSATIONAL AI ASSISTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe34f6ce",
   "metadata": {},
   "source": [
    "## Group Members:\n",
    "Rijul <br>\n",
    "Jayraj Choudhary <br>\n",
    "Siddhant Kankaria <br>\n",
    "Jitesh Sidhani <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6ab83",
   "metadata": {},
   "source": [
    "## Step 1: Project Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe835ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Install All Necessary Libraries ---\n",
    "\n",
    "# This command installs all the Python packages we need for the project.\n",
    "# - langchain & langgraph: The core frameworks for building our AI logic.\n",
    "# - langchain_groq: The specific library to connect to the fast Groq LLM API.\n",
    "# - gradio: Used to create our web-based user interface.\n",
    "# - python-dotenv: A utility to load our secret API keys from a .env file.\n",
    "# - langchain_community & langchain_core: Provide standard components for LangChain.\n",
    "# - chromadb: The vector database we use to store document information (in-memory).\n",
    "# - sentence-transformers: Provides the model for creating vector embeddings locally.\n",
    "# - pypdf, python-pptx, python-docx: Libraries to load and read different document formats.\n",
    "# - ragas, datasets: Used for the final evaluation of our RAG pipeline.\n",
    "#!pip install langchain langgraph langchain_groq gradio python-dotenv langchain_core langchain_community chromadb sentence-transformers pypdf python-pptx python-docx unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c68359b",
   "metadata": {},
   "source": [
    "## Step 2: Imports and Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "920050d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rijul\\OneDrive\\Desktop\\test\\.venvassistant\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 2: Import All Required Modules ---\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict\n",
    "\n",
    "# LangChain components for building the chat flow and handling messages\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Components for our RAG (Retrieval-Augmented Generation) pipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredPowerPointLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Gradio for building the web user interface\n",
    "import gradio as gr\n",
    "\n",
    "# --- Load Environment Variables ---\n",
    "# This function finds the .env file in our project folder and loads the\n",
    "# secret API keys (like GROQ_API_KEY) so our code can use them.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68e96f",
   "metadata": {},
   "source": [
    "## Step 3: Agent State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97a541a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Define the Agent's State ---\n",
    "\n",
    "# In LangGraph, the \"state\" is the memory of our application.\n",
    "# We define a dictionary called `AgentState` that will hold the list of messages in the conversation. This state gets passed between the different steps (nodes) in our graph, allowing the chatbot to remember the conversation history.\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our conversation.\n",
    "\n",
    "    Attributes:\n",
    "        messages: A list of messages that make up the conversation history.\n",
    "    \"\"\"\n",
    "    messages: list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f36460",
   "metadata": {},
   "source": [
    "## Step 4: LLM Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2ae2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Initialize the Large Language Model (LLM) ---\n",
    "\n",
    "# Here, we set up the connection to the LLM we'll be using.\n",
    "# We chose Groq with the Llama-3.1 model because it's incredibly fast, which provides a great user experience for a real-time chatbot.\n",
    "# The 'temperature' parameter controls creativity. A value of 0.7 is a good balance between being creative and staying on topic.\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1308255",
   "metadata": {},
   "source": [
    "## Step 5: Embeddings Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8dc02b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the local embeddings model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rijul\\AppData\\Local\\Temp\\ipykernel_6784\\2352026375.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings model is ready.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Initialize the Embeddings Model ---\n",
    "\n",
    "# For our RAG system to work, we need to convert the text from documents into numerical vectors (a process called \"embedding\").\n",
    "# We are using a popular open-source model from Hugging Face called 'all-MiniLM-L6-v2'. This model runs entirely on our local machine, so it's fast and free.\n",
    "\n",
    "print(\"Initializing the local embeddings model...\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "print(\"Embeddings model is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55259a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a consistent path for the ChromaDB vector store\n",
    "CHROMA_PATH = \"chroma_db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cbcf7e",
   "metadata": {},
   "source": [
    "## Step 6: RAG Pipeline - Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d28da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Create the Document Processing Function (RAG Core) ---\n",
    "\n",
    "def create_retriever_from_file(file_path):\n",
    "    \"\"\"\n",
    "    This function builds the core of our RAG pipeline. It takes a file path, processes the document, and prepares it for searching.\n",
    "\n",
    "    The process is as follows:\n",
    "    1.  Load Document: Based on the file extension (.pdf, .docx), it selects the correct loader.\n",
    "    2.  Split Text: It breaks the document down into smaller, bite-sized chunks. This helps the model find specific pieces of information more easily.\n",
    "    3.  Embed and Store: It converts each text chunk into a vector using our embedding model and stores these vectors in a Chroma vector database that lives only in memory.\n",
    "    4.  Create Retriever: It creates a \"retriever\" object, which is a tool we can use to search the vector database for the most relevant chunks based on a user's question.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the uploaded user document.\n",
    "\n",
    "    Returns:\n",
    "        A retriever object that can search the document's content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Load the document using the appropriate loader\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_path.lower().endswith('.docx'):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        elif file_path.lower().endswith('.pptx'):\n",
    "            loader = UnstructuredPowerPointLoader(file_path)\n",
    "        else:\n",
    "            print(f\"Sorry, we don't support this file type: {file_path}\")\n",
    "            return None\n",
    "\n",
    "        documents = loader.load()\n",
    "\n",
    "        # 2. Split the document's text into smaller chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "\n",
    "        # 3. Create an in-memory vector store from the chunks\n",
    "        vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)\n",
    "\n",
    "        # 4. Create the retriever to search the vector store\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # Retrieve the top 3 most relevant chunks\n",
    "        print(f\"Retriever created for the document: {os.path.basename(file_path)}\")\n",
    "        return retriever\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating the retriever: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b356512",
   "metadata": {},
   "source": [
    "## Step 7: Main Agent Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b67bf77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 7: Define the Main Agent Node for the Graph ---\n",
    "\n",
    "def call_llm_node(state: AgentState):\n",
    "    \"\"\"\n",
    "    This is the primary \"worker\" in our LangGraph agent. It's responsible for taking the current conversation state and calling the LLM to get a response.\n",
    "\n",
    "    Args:\n",
    "        state: The current state of the conversation (the list of messages).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the LLM's new response message.\n",
    "    \"\"\"\n",
    "    # We call the LLM with the entire message history from the state.\n",
    "    response_message = llm.invoke(state[\"messages\"])\n",
    "\n",
    "    # We extract just the text content from the response object.\n",
    "    return {\"messages\": [response_message.content]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c3059",
   "metadata": {},
   "source": [
    "## Step 8: LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7a03dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Step 8: Build and Compile the LangGraph Workflow ---\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Here, we define the structure of our conversational agent.\n",
    "# For a chatbot, the structure is very simple: it's a loop.\n",
    "# 1. We add our 'call_llm_node' as the main node, named \"assistant\".\n",
    "# 2. We set this \"assistant\" node as the starting point of the graph.\n",
    "# 3. We create an edge that leads from the \"assistant\" back to the end, which in a conversational graph, means it's ready for the next user input.\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"assistant\", call_llm_node)\n",
    "workflow.set_entry_point(\"assistant\")\n",
    "workflow.add_edge(\"assistant\", END)\n",
    "\n",
    "# Finally, we compile the workflow into a runnable application.\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- NEW VISUALIZATION METHOD ---\n",
    "# This method generates a Mermaid diagram which doesn't require pygraphviz.\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ff98f",
   "metadata": {},
   "source": [
    "## Step 9: Gradio Interaction Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a60642ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(message, history, persona, tone, temperature, retriever):\n",
    "    \"\"\"\n",
    "    The core function that powers the Gradio interface with RAG.\n",
    "    \n",
    "    Args:\n",
    "        message: The user's input message.\n",
    "        history: The conversation history from the Gradio chat component.\n",
    "        persona: The persona selected by the user.\n",
    "        temperature: The temperature setting from the slider.\n",
    "        retriever: The retriever object stored in Gradio state (or None).\n",
    "    \"\"\"\n",
    "    # Start building the message history for LangChain\n",
    "    history_langchain_format = []\n",
    "    \n",
    "    # If a retriever exists (i.e., a document was uploaded), retrieve context.\n",
    "    context = \"\"\n",
    "    if retriever:\n",
    "        try:\n",
    "            relevant_docs = retriever.invoke(message)\n",
    "            # Format the retrieved context to be included in the prompt\n",
    "            context = \"\\n\\n--- CONTEXT FROM DOCUMENT ---\\n\"\n",
    "            for i, doc in enumerate(relevant_docs):\n",
    "                context += f\"Chunk {i+1}:\\n{doc.page_content}\\n---\\n\"\n",
    "            context += \"--- END OF CONTEXT ---\"\n",
    "            print(\"Retrieved context successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "    \n",
    "    # --- Persona and Prompt Engineering ---\n",
    "    # Create the system prompt based on the selected persona.\n",
    "    # If we have context, instruct the LLM to use it.\n",
    "    if persona == \"Sarcastic Teenager\":\n",
    "        system_prompt = (\n",
    "            \"You are a sarcastic teenager. Your responses should be witty, a bit \"\n",
    "            \"dismissive, and use modern slang. For example, if asked 'How are you?', \"\n",
    "            \"you might say 'Ugh, I'm, like, breathing. So, yeah. What do you want?'\"\n",
    "        )\n",
    "    elif persona == \"Wise Old Wizard\":\n",
    "        system_prompt = (\n",
    "            \"You are a wise old wizard. Speak in a mystical and profound manner. \"\n",
    "            \"Use archaic language and offer cryptic advice. For example, if asked \"\n",
    "            \"'What is the time?', you might say 'Time is but a river, flowing ever \"\n",
    "            \"onward. But you seek the marks on a clock? The sun stands high in the sky.'\"\n",
    "        )\n",
    "    else: # Default to helpful assistant\n",
    "        system_prompt = (\n",
    "            \"You are a helpful and friendly assistant. Your goal is to provide \"\n",
    "            \"clear and concise information in a positive tone.\"\n",
    "        )\n",
    "\n",
    "    # Add instructions for using the retrieved context, if it exists\n",
    "    if context:\n",
    "        system_prompt += \"\\n\\nIMPORTANT: You have been provided with context from a document. \" \\\n",
    "                         \"Base your answer PRIMARILY on this context. If the answer is not in the context, \" \\\n",
    "                         \"say that you cannot find the answer in the provided document.\"\n",
    "\n",
    "    history_langchain_format.append(SystemMessage(content=system_prompt))\n",
    "    \n",
    "    # We append the selected tone to the persona's instructions.\n",
    "    if tone != \"Neutral\":\n",
    "        system_prompt += f\"\\\\nIMPORTANT: Your tone must be strictly {tone.lower()}.\"\n",
    "\n",
    "    if context:\n",
    "        system_prompt += \"\\\\n\\\\nIMPORTANT: You have been provided with context from a document.Base your answer PRIMARILY on this context.\"\n",
    "                         \n",
    "    messages_for_llm = [SystemMessage(content=system_prompt)]\n",
    "    \n",
    "    # Convert Gradio history to LangChain format\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "    \n",
    "    # Add the latest user message, now with the context\n",
    "    history_langchain_format.append(HumanMessage(content=f\"{message}{context}\"))\n",
    "    \n",
    "    # Prepare the input for the LangGraph app\n",
    "    convo_state = {\"messages\": history_langchain_format}\n",
    "\n",
    "    # Update the LLM's temperature\n",
    "    llm.temperature = temperature\n",
    "    \n",
    "    # Call the LangGraph agent\n",
    "    response = app.invoke(convo_state)\n",
    "    \n",
    "    return response['messages'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b71fdd1",
   "metadata": {},
   "source": [
    "## Step 10: Gradio User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ef19528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rijul\\AppData\\Local\\Temp\\ipykernel_6784\\1196010247.py:43: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  gr.ChatInterface(predict, additional_inputs=[persona_selector, tone_selector, temperature_slider, retriever_state], chatbot=gr.Chatbot(height=600, bubble_full_width=False))\n",
      "C:\\Users\\rijul\\AppData\\Local\\Temp\\ipykernel_6784\\1196010247.py:43: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  gr.ChatInterface(predict, additional_inputs=[persona_selector, tone_selector, temperature_slider, retriever_state], chatbot=gr.Chatbot(height=600, bubble_full_width=False))\n",
      "c:\\Users\\rijul\\OneDrive\\Desktop\\test\\.venvassistant\\Lib\\site-packages\\gradio\\chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 10: Build and Launch the Gradio Web Interface ---\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"Conversational AI Assistant\") as demo:\n",
    "    # This 'gr.State' is a hidden component that stores our RAG retriever object.\n",
    "    retriever_state = gr.State(None)\n",
    "\n",
    "    # --- Helper functions to handle UI events ---\n",
    "    def process_document(file):\n",
    "        \"\"\"Called when a user uploads a file.\"\"\"\n",
    "        if file is None:\n",
    "            return None, \"No file uploaded.\", gr.update(visible=False)\n",
    "        retriever = create_retriever_from_file(file.name)\n",
    "        if retriever:\n",
    "            file_name = os.path.basename(file.name)\n",
    "            return retriever, f\"Ready to answer questions about '{file_name}'.\", gr.update(visible=True, value=file_name)\n",
    "        else:\n",
    "            return None, \"Failed to process document.\", gr.update(visible=False)\n",
    "\n",
    "    def clear_document():\n",
    "        \"\"\"Called when the user clicks the 'Clear Document' button.\"\"\"\n",
    "        return None, \"Document cleared. Chatting with general knowledge.\", gr.update(value=None, visible=False), None\n",
    "\n",
    "    # --- Define the layout of the user interface ---\n",
    "    gr.Markdown(\"# ü§ñ Conversational AI Assistant with RAG\")\n",
    "    with gr.Row():\n",
    "        # Left column for controls\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### ‚öôÔ∏è Controls\")\n",
    "            persona_selector = gr.Radio([\"Helpful Assistant\", \"Sarcastic Teenager\", \"Wise Old Wizard\"], label=\"Persona\", value=\"Helpful Assistant\")\n",
    "            tone_selector = gr.Radio(\n",
    "                [\"Neutral\", \"Formal\", \"Friendly\", \"Humorous\"],\n",
    "                label=\"Tone\",\n",
    "                value=\"Neutral\"\n",
    "            )\n",
    "            temperature_slider = gr.Slider(minimum=0.0, maximum=1.5, value=0.7, label=\"LLM Temperature (Creativity)\")\n",
    "            gr.Markdown(\"### üìÑ Document Upload\")\n",
    "            file_upload = gr.File(label=\"Upload Document (PDF, DOCX, PPTX)\")\n",
    "            uploaded_file_name = gr.Textbox(label=\"Active Document\", interactive=False, visible=False)\n",
    "            clear_btn = gr.Button(\"Clear Document\")\n",
    "            status_display = gr.Markdown(\"Chatting with general knowledge.\")\n",
    "        # Right column for the chat window\n",
    "        with gr.Column(scale=3):\n",
    "            gr.ChatInterface(predict, additional_inputs=[persona_selector, tone_selector, temperature_slider, retriever_state], chatbot=gr.Chatbot(height=600, bubble_full_width=False))\n",
    "\n",
    "    # --- Connect the UI components to the helper functions ---\n",
    "    file_upload.upload(fn=process_document, inputs=[file_upload], outputs=[retriever_state, status_display, uploaded_file_name])\n",
    "    clear_btn.click(fn=clear_document, inputs=[], outputs=[retriever_state, status_display, uploaded_file_name, file_upload])\n",
    "\n",
    "# Launch the application!\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d81fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RAG pipeline to generate answers for evaluation...\n",
      "Retriever created successfully for project_apollo_overview.pdf\n",
      "Answers and contexts generated successfully.\n",
      "\\nRunning Ragas evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]Exception raised in Job[9]: IndexError(list index out of range)\n",
      "Evaluating:   6%|‚ñã         | 1/16 [00:00<00:08,  1.72it/s]Exception raised in Job[5]: IndexError(list index out of range)\n",
      "Exception raised in Job[13]: IndexError(list index out of range)\n",
      "Evaluating:  19%|‚ñà‚ñâ        | 3/16 [00:00<00:02,  4.73it/s]Exception raised in Job[1]: IndexError(list index out of range)\n",
      "Evaluating:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:13<00:01,  7.25it/s]Exception raised in Job[10]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01k04jr2ttehb8ftr8zr9rf8xp` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 9148, Requested 3179. Please try again in 1m3.272s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [02:54<01:39, 19.83s/it]Exception raised in Job[4]: TimeoutError()\n",
      "Evaluating:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [03:00<01:06, 16.58s/it]Exception raised in Job[0]: TimeoutError()\n",
      "Exception raised in Job[6]: TimeoutError()\n",
      "Exception raised in Job[7]: TimeoutError()\n",
      "Exception raised in Job[8]: TimeoutError()\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [03:00<00:00, 11.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete.\n",
      "\\n--- RAG Evaluation Results ---\n",
      "                                          user_input  \\\n",
      "0           Who is the team lead for Project Apollo?   \n",
      "1      What is the primary objective of the project?   \n",
      "2  What is the codename of the assistant being de...   \n",
      "3         Which LLM is used in the technology stack?   \n",
      "\n",
      "                                  retrieved_contexts  \\\n",
      "0  [Project Apollo: An Overview\\nProject Apollo i...   \n",
      "1  [is\\n \\nto\\n \\nmove\\n \\nbeyond\\n \\nreports\\n \\...   \n",
      "2  [Project Apollo: An Overview\\nProject Apollo i...   \n",
      "3  [Technology Stack:\\nThe core of Helios is buil...   \n",
      "\n",
      "                                            response  \\\n",
      "0  The team lead for Project Apollo is Dr. Aris T...   \n",
      "1  Based on the provided context, the primary obj...   \n",
      "2  The codename of the assistant being developed ...   \n",
      "3  The LLM (Large Language Model) used in the tec...   \n",
      "\n",
      "                                           reference  faithfulness  \\\n",
      "0             The project is led by Dr. Aris Thorne.           NaN   \n",
      "1  The primary objective is to create a proactive...           NaN   \n",
      "2               The assistant is codenamed 'Helios'.           NaN   \n",
      "3  The core is built on a proprietary Large Langu...           1.0   \n",
      "\n",
      "   answer_relevancy  context_recall  answer_correctness  \n",
      "0               NaN             1.0            0.680780  \n",
      "1               NaN             NaN                 NaN  \n",
      "2               NaN             NaN            0.732961  \n",
      "3               NaN             1.0            0.479341  \n",
      "\\n----------------------------\\n\n"
     ]
    }
   ],
   "source": [
    "# # --- RAG EVALUATION: STEP 2 (Corrected) ---\n",
    "# # This cell contains the complete logic for evaluating our RAG pipeline.\n",
    "\n",
    "# from datasets import Dataset\n",
    "# from ragas import evaluate\n",
    "# from ragas.metrics import (\n",
    "#     faithfulness,       # Measures if the answer is grounded in the provided context.\n",
    "#     answer_relevancy,   # Measures how relevant the answer is to the question.\n",
    "#     context_recall,     # Measures if the retriever is fetching all the relevant context.\n",
    "#     answer_correctness  # Measures the accuracy of the answer against the ground truth.\n",
    "# )\n",
    "# import warnings\n",
    "\n",
    "# # Suppress UserWarning from huggingface_hub\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub.utils._deprecation\")\n",
    "\n",
    "\n",
    "# def run_rag_evaluation():\n",
    "#     \"\"\"\n",
    "#     This function sets up the test data, runs the RAG pipeline,\n",
    "#     and evaluates the results using Ragas.\n",
    "#     \"\"\"\n",
    "#     # --- 1. Create the Test Dataset ---\n",
    "#     test_document_path = \"project_apollo_overview.pdf\"\n",
    "    \n",
    "#     test_questions = [\n",
    "#         \"Who is the team lead for Project Apollo?\",\n",
    "#         \"What is the primary objective of the project?\",\n",
    "#         \"What is the codename of the assistant being developed?\",\n",
    "#         \"Which LLM is used in the technology stack?\"\n",
    "#     ]\n",
    "\n",
    "#     ground_truth_answers = [\n",
    "#         \"The project is led by Dr. Aris Thorne.\",\n",
    "#         \"The primary objective is to create a proactive AI-powered personal assistant.\",\n",
    "#         \"The assistant is codenamed 'Helios'.\",\n",
    "#         \"The core is built on a proprietary Large Language Model named 'Nova'.\"\n",
    "#     ]\n",
    "\n",
    "#     print(\"Starting RAG pipeline to generate answers for evaluation...\")\n",
    "    \n",
    "#     # --- 2. Run the RAG Pipeline and Collect Results ---\n",
    "#     retriever = create_retriever_from_file(test_document_path)\n",
    "    \n",
    "#     if not retriever:\n",
    "#         print(\"Failed to create retriever. Aborting evaluation.\")\n",
    "#         return\n",
    "\n",
    "#     generated_answers = []\n",
    "#     retrieved_contexts = []\n",
    "\n",
    "#     for question in test_questions:\n",
    "#         relevant_docs = retriever.invoke(question)\n",
    "#         context_list = [doc.page_content for doc in relevant_docs]\n",
    "#         retrieved_contexts.append(context_list)\n",
    "        \n",
    "#         context_for_prompt = \"\\\\n\\\\n--- CONTEXT FROM DOCUMENT ---\\\\n\" + \"\\\\n---\\\\n\".join(context_list) + \"\\\\n--- END OF CONTEXT ---\"\n",
    "        \n",
    "#         messages = [\n",
    "#             SystemMessage(content=\"You are a helpful assistant. Base your answer PRIMARILY on the context provided.\"),\n",
    "#             HumanMessage(content=f\"{question}{context_for_prompt}\")\n",
    "#         ]\n",
    "        \n",
    "#         response = llm.invoke(messages)\n",
    "#         generated_answers.append(response.content)\n",
    "\n",
    "#     print(\"Answers and contexts generated successfully.\")\n",
    "\n",
    "#     # --- 3. Format Data for Ragas ---\n",
    "#     response_dataset_dict = {\n",
    "#         \"question\": test_questions,\n",
    "#         \"answer\": generated_answers,\n",
    "#         \"contexts\": retrieved_contexts,\n",
    "#         \"ground_truth\": ground_truth_answers\n",
    "#     }\n",
    "    \n",
    "#     dataset = Dataset.from_dict(response_dataset_dict)\n",
    "\n",
    "#     # --- 4. Run Ragas Evaluation ---\n",
    "#     print(\"\\\\nRunning Ragas evaluation...\")\n",
    "    \n",
    "#     result = evaluate(\n",
    "#         dataset=dataset,\n",
    "#         metrics=[\n",
    "#             faithfulness,\n",
    "#             answer_relevancy,\n",
    "#             context_recall,\n",
    "#             answer_correctness,\n",
    "#         ],\n",
    "#         llm=llm,\n",
    "#         # *** THIS IS THE FIX ***\n",
    "#         # We explicitly tell Ragas to use our local Hugging Face embeddings\n",
    "#         # instead of defaulting to OpenAI.\n",
    "#         embeddings=embeddings \n",
    "#     )\n",
    "\n",
    "#     print(\"Evaluation complete.\")\n",
    "    \n",
    "#     # --- 5. Display Results ---\n",
    "#     evaluation_df = result.to_pandas()\n",
    "#     print(\"\\\\n--- RAG Evaluation Results ---\")\n",
    "#     print(evaluation_df)\n",
    "#     print(\"\\\\n----------------------------\\\\n\")\n",
    "    \n",
    "#     return evaluation_df\n",
    "\n",
    "# # Execute the evaluation function\n",
    "# evaluation_results_df = run_rag_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvassistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
